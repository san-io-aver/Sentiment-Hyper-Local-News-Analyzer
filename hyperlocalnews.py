# -*- coding: utf-8 -*-
"""HyperLocalNews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18GchsiGWG9XAeon3Jx5IIO3ZqjZWZJdY
"""

!pip install streamlit

import requests
from bs4 import BeautifulSoup

city = input("Enter city name: ")
city = city.strip().lower()
city

"""links of sites

"""

Sites = {
    "TOI": {
        "url": f"https://timesofindia.indiatimes.com/city/{city}",
        "article_selector": "a",
        "filter": lambda tag: "articleshow" in tag.get("href", ""),
        "base_url": "https://timesofindia.indiatimes.com"
    },
    "NDTV": {
        "url": f"https://www.ndtv.com/{city}-news#pfrom=home-ndtv_mainnavigation",
        "article_selector": "h2",
        "filter": lambda tag: tag.find("a") and tag.find("a").has_attr("href"),
        "base_url": "https://www.ndtv.com"
    }
}

def fetch_news(data):
    url = data["url"]
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    articles = []

    for tag in soup.find_all(data["article_selector"]):
        if data["filter"](tag):

            a_tag = tag if tag.name == "a" else tag.find("a")
            if not a_tag:
                continue

            href = a_tag.get("href")
            title = a_tag.get_text(strip=True)

            if href and title and len(title)>30:
                full_link = href if href.startswith("http") else data["base_url"] + href
                articles.append((title, full_link))

    return articles

for name, data in Sites.items():
  print(f"Scraping {name}....")
  articles = fetch_news(data)
  for title,link in articles:
    print(f"{title}: {link}")

